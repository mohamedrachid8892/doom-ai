{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3e8a33-6fc3-4881-855f-185032e7a9c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Get VizDoom Up and Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d72d31-3b43-49c5-9fd4-26f9b172d647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2858ba5-8a11-4e66-b166-81b3dd471054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3994bc91-d4c3-403f-b86a-0d9fa58c59cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import vizdoom for game environment\n",
    "from vizdoom import *\n",
    "# Import random to take random actions\n",
    "import random\n",
    "# Import time to slow down game, sleep between frames\n",
    "import time\n",
    "# Import numpy for identity matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901405e-8dd6-4a73-bb6a-779db1b45715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Game\n",
    "game = DoomGame()\n",
    "game.load_config('github/ViZDoom/scenarios/deadly_corridor_s1.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f394bfb-ce13-437b-98c1-d50923601ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the actions we can take in the environment - Move left, move right, attack\n",
    "actions = np.identity(7, dtype=np.uint8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575bc05-262b-48d9-bfd8-2f53a3a8dd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2108a0b-a006-4076-b25d-232301c9110e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state.game_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdae2b0-ba77-49b4-b1b0-53e98770620c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fffb8-5d08-4b44-9c5f-5c76cf239a07",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through episodes\n",
    "episodes = 10 # Number of games to play\n",
    "for episode in range(episodes):\n",
    "    # Create new episode or game\n",
    "    game.new_episode()\n",
    "    # Check that the game isn't done\n",
    "    while not game.is_episode_finished():\n",
    "        # Get the game state\n",
    "        state = game.get_state()\n",
    "        # Get the game image\n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables (in this case, ammo)\n",
    "        info = state.game_variables\n",
    "        # Take an action. Pass in frame skip to give AI time to process each action reward\n",
    "        reward = game.make_action(random.choice(actions), 4)\n",
    "        # Print the reward for each action\n",
    "        # print('Reward:', reward)\n",
    "        time.sleep(0.02)\n",
    "    # Print total reward for full game\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c2ce0-96a8-44c0-9d72-562ee22f5da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026b1c4-52c6-4b0c-ad28-9b4f04b87be9",
   "metadata": {},
   "source": [
    "# 2. Converting the Environment to a Gym Envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aaaecf-12e5-4ac9-9eab-272000d7723a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bac96f0-76a0-4826-bddd-1fc6332ca1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gym import Env\n",
    "# Import gym spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "# Import opencv, Used to greyscale observations to make processing environment faster\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c715b10-23ac-4e4f-8a38-d76bbea4e096",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create VIZDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env):\n",
    "    \n",
    "    # Function that is called when we start the environment\n",
    "    def __init__(self, render=False, config='github/ViZDoom/scenarios/deadly_corridor_s1.cfg'):\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup the game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config) # Pass in whatever environment you need from the scenarios folder.\n",
    "        \n",
    "        # Game Variables:\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52\n",
    "        \n",
    "        # Define whether or not to render the game window.\n",
    "        # Rendering the window takes away from computing power, so disabling is ideal for testing\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "    \n",
    "    # How we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step within game\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        movement_reward = self.game.make_action(actions[action], 4)\n",
    "        \n",
    "        reward = 0\n",
    "        # Get all other stuff we need to return\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.greyscale(state)\n",
    "            \n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "            \n",
    "            # Calculate reward deltas\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200 + ammo_delta*5\n",
    "            info = ammo\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0\n",
    "            \n",
    "        info = {\"info\": info}\n",
    "            \n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info\n",
    "    \n",
    "    # Define how to render the game or environment. ViZDOom already defines this for us, so just pass.\n",
    "    def render():\n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new gmae\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.greyscale(state)\n",
    "    \n",
    "    # Custom function. Greyscale the game frame and resize it\n",
    "    def greyscale(self, observation):\n",
    "        # Reshape the observation array for cvtColor and change color channels\n",
    "        grey = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the image and scale down so there are less pixels to process\n",
    "        resize = cv2.resize(grey, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100, 160, 1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f59f82-04e8-4a83-b08a-68f7fb33336b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b4cdf-d439-4214-b9c2-492bc66f4e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7342b8-d2d9-4ad0-8dc1-af9492fe8c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7be77c-5454-4f1f-823e-c9a3e256ebdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e0ece-bc75-4476-b422-7f649eb21580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb60e5-300a-4db1-ba42-b7e632948217",
   "metadata": {},
   "source": [
    "# 3. View State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95645a20-1b3d-423f-afae-9fa0ab926ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e51f0a-2e6d-4d24-843f-85054082d69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257ff4d-cdff-4eb4-b97d-4e9a16eacd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(state, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c7913-a145-42d0-9633-7768a49846d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Log the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f72c4c-f4da-4a97-88ee-80ae5635cbaa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6896a25-94fe-4d58-b4ab-6bd49134a9b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7591c06-e1c1-4fa6-8664-a5c8bfa6bcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import os for file navigation\n",
    "import os\n",
    "# Import callback class from stable baselines 3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0242c4f-0ef5-438b-aa80-39ced1dbe050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3ff888-d59f-4895-94b5-f56c11c1e759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_deadly_corridor'\n",
    "LOG_DIR = './logs/log_deadly_corridor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be35ef98-40ae-44c9-a5aa-791bf49baa57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3e0ee-3b14-47fb-b380-c215d5f51484",
   "metadata": {},
   "source": [
    "# 5. Train Model using Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6137cab1-18c1-44cb-a43f-2dc3d4b27ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the PPO algorithm for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "931f1e85-6d8d-4549-aa47-ed08293f6161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = VizDoomGym(config='github/ViZDoom/scenarios/deadly_corridor_s2.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "341faec1-c18f-4df6-a627-640438a978a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# CnnPolicy because we are passing in an image\n",
    "# Cnn = Convolution neural network\n",
    "\n",
    "# model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=4096)\n",
    "# Change hyperparameters for more stability in model training\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=0.95, gae_lambda=.9)\n",
    "# model = PPO.load('./train/train_deadly_corridor/best_model_560000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c0d31f-71b7-44cc-bae6-bf5f0dc9a9f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_deadly_corridor\\PPO_13\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.7     |\n",
      "|    ep_rew_mean     | 55.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 56       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 77.5         |\n",
      "|    ep_rew_mean          | 58           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 58           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024412374 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | -4.18e-05    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.16e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    value_loss           | 1.65e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 78           |\n",
      "|    ep_rew_mean          | 86.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 55           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 439          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022399318 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0222       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.51e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000544    |\n",
      "|    value_loss           | 1.76e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    247\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 250\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mVizDoomGym.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Specify action and take step within game\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m7\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 35\u001b[0m     movement_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Get all other stuff we need to return\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=80000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f1a696-ff1a-4e1e-9da5-3e74f7ffc20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4f21d-c88c-4857-8699-cf77c08ba64f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = VizDoomGym(config='github/ViZDoom/scenarios/deadly_corridor_s2.cfg')\n",
    "model.set_env(env)\n",
    "model = PPO.load('./train/train_deadly_corridor/best_model_340000')\n",
    "model.learn(total_timesteps=40000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899b2f4-c91f-4617-95a1-6f60680a16d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = VizDoomGym(config='github/ViZDoom/scenarios/deadly_corridor_s3.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99b085-4c41-4167-99f9-834fa35bb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(config='github/ViZDoom/scenarios/deadly_corridor_s4.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=40000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84121e7f-81e9-47ef-8faf-857f0a80d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(config='github/ViZDoom/scenarios/deadly_corridor_s5.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=40000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8574072-c7ea-4146-bad4-d5638704de87",
   "metadata": {
    "tags": []
   },
   "source": [
    "To view the logs run `tensorboard --logdir=.` in the PPO dir of the model run.  \n",
    "  \n",
    "Explaining the data:  \n",
    "> 1. `ep_len_mean`: Mean episode length (averaged over stats_window_size episodes, 100 by default)\n",
    "> 2. `ep_rew_mean`: Mean episodic training reward (averaged over stats_window_size episodes, 100 by default).\n",
    "> 3. `approx_kl`: approximate mean KL divergence between old and new policy (for PPO), it is an estimation of how much changes happened in the update\n",
    "> 4. `clip_fraction`: mean fraction of surrogate loss that was clipped (above clip_range threshold) for PPO.\n",
    "> 5. `clip_range`: Current value of the clipping factor for the surrogate loss of PPO\n",
    "> 6. `critic_loss`: Current value for the critic function loss for off-policy algorithms, usually error between value function output and TD(0), temporal difference estimate\n",
    "> 7. `learning_rate`: Current learning rate value\n",
    "> 8. `loss`: Current total loss value\n",
    "> 9. `n_updates`: Number of gradient updates applied so far\n",
    "> 14. `policy_gradient_loss`: Current value of the policy gradient loss (its value does not have much meaning)\n",
    "> 15. `value_loss`: Current value for the value function loss for on-policy algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbeb17f-924c-40ad-a36a-f2a6f5a26be5",
   "metadata": {},
   "source": [
    "# 6. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0ba5b-b920-43cc-ae54-85de3ca226db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca591ce-a954-418e-bbcd-ca651961e90f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "# model = PPO.load('./train/train_deadly_corridor/best_model_560000_nick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807842e-2155-444b-830f-f834723646e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True, config=\"github/ViZDoom/scenarios/deadly_corridor_s3.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d95c2-acf7-4e33-87d8-855f66e382a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 100 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a004cba-0244-47fd-9d3b-970083faf02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99ad8b-e35f-49db-acb9-ef960d8d8d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # time.sleep(0.125)\n",
    "        total_reward += reward\n",
    "    print('Total reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89174c2-3a1c-4d60-93f7-d2b212d2c152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5b7b1-5cc0-454b-b8d4-c95066c3cb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
